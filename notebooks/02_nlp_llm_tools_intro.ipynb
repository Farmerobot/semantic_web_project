{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and LLM Tools Introduction\n",
    "\n",
    "This notebook demonstrates the basic usage of NLP and LLM tools used in the Persuasion-Aware MUSE project:\n",
    "\n",
    "1. **spaCy** - Industrial-strength NLP library for entity recognition and text processing\n",
    "2. **OpenRouter API** - LLM gateway for accessing Gemini and other models\n",
    "\n",
    "**Model Used**: `google/gemini-2.5-flash-lite` via OpenRouter\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy openai python-dotenv\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model loaded successfully\")\n",
    "except OSError:\n",
    "    print(\"Please run: python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. spaCy for Named Entity Recognition (NER)\n",
    "\n",
    "spaCy provides pre-trained models for extracting named entities from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities Found:\n",
      "============================================================\n",
      "Text: The European Union             | Label: ORG             | Description: Companies, agencies, institutions, etc.\n",
      "Text: Germany                        | Label: GPE             | Description: Countries, cities, states\n",
      "Text: 500,000 migrants               | Label: QUANTITY        | Description: Measurements, as of weight or distance\n",
      "Text: Angela Merkel                  | Label: PERSON          | Description: People, including fictional\n",
      "Text: WHO                            | Label: ORG             | Description: Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "# Sample social media post\n",
    "sample_post = \"\"\"\n",
    "BREAKING: The European Union is forcing Germany to accept 500,000 migrants! \n",
    "Angela Merkel knew this would happen. COVID-19 was just a distraction created by WHO.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(sample_post)\n",
    "\n",
    "# Extract named entities\n",
    "print(\"Named Entities Found:\")\n",
    "print(\"=\" * 60)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Text: {ent.text:30} | Label: {ent.label_:15} | Description: {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Entity Classification for Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Entities for Knowledge Graph:\n",
      "============================================================\n",
      "Name: The European Union        | Type: Organization   \n",
      "Name: Germany                   | Type: Location       \n",
      "Name: 500,000 migrants          | Type: Other          \n",
      "Name: Angela Merkel             | Type: Person         \n",
      "Name: WHO                       | Type: Organization   \n"
     ]
    }
   ],
   "source": [
    "def extract_entities_for_kg(text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract named entities from text and map to knowledge graph entity types.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Map spaCy labels to our ontology types\n",
    "    label_mapping = {\n",
    "        \"PERSON\": \"Person\",\n",
    "        \"ORG\": \"Organization\",\n",
    "        \"GPE\": \"Location\",\n",
    "        \"LOC\": \"Location\",\n",
    "        \"NORP\": \"Group\",\n",
    "        \"EVENT\": \"Event\",\n",
    "        \"DATE\": \"Date\",\n",
    "    }\n",
    "    \n",
    "    entities = []\n",
    "    seen = set()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.text.lower() not in seen:\n",
    "            seen.add(ent.text.lower())\n",
    "            entity_type = label_mapping.get(ent.label_, \"Other\")\n",
    "            entities.append({\n",
    "                \"name\": ent.text,\n",
    "                \"type\": entity_type,\n",
    "                \"original_label\": ent.label_\n",
    "            })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Test the function\n",
    "entities = extract_entities_for_kg(sample_post)\n",
    "print(\"Extracted Entities for Knowledge Graph:\")\n",
    "print(\"=\" * 60)\n",
    "for ent in entities:\n",
    "    print(f\"Name: {ent['name']:25} | Type: {ent['type']:15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. OpenRouter API for LLM-based Analysis\n",
    "\n",
    "We use OpenRouter to access various LLM models. For this project, we use **Google Gemini 2.5 Flash Lite** for:\n",
    "- Claim extraction\n",
    "- Persuasion technique detection\n",
    "- Fact verification assistance\n",
    "\n",
    "OpenRouter provides an OpenAI-compatible API, making it easy to switch between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup OpenRouter Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter client initialized successfully\n",
      "Using model: google/gemini-2.5-flash-lite\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# OpenRouter configuration\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "MODEL_NAME = \"google/gemini-2.5-flash-lite\"\n",
    "\n",
    "# Initialize client with OpenRouter\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if api_key:\n",
    "    client = OpenAI(\n",
    "        base_url=OPENROUTER_BASE_URL,\n",
    "        api_key=api_key\n",
    "    )\n",
    "    print(f\"OpenRouter client initialized successfully\")\n",
    "    print(f\"Using model: {MODEL_NAME}\")\n",
    "else:\n",
    "    print(\"Warning: OPENROUTER_API_KEY not found in environment\")\n",
    "    print(\"Please set your API key in a .env file:\")\n",
    "    print(\"  OPENROUTER_API_KEY=your-key-here\")\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Claim Extraction with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Claims:\n",
      "============================================================\n",
      "\n",
      "Claim 1:\n",
      "  Text: The European Union is forcing Germany to accept 500,000 migrants!\n",
      "  Verifiable: True\n",
      "\n",
      "Claim 2:\n",
      "  Text: Angela Merkel knew this would happen.\n",
      "  Verifiable: True\n",
      "\n",
      "Claim 3:\n",
      "  Text: COVID-19 was just a distraction created by WHO.\n",
      "  Verifiable: True\n"
     ]
    }
   ],
   "source": [
    "def extract_claims_llm(text: str, client) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract factual claims from text using LLM via OpenRouter.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        return [{\"error\": \"OpenRouter client not initialized\"}]\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the following social media post and extract all factual claims that can be verified.\n",
    "For each claim, provide:\n",
    "1. The exact text of the claim\n",
    "2. A brief description\n",
    "3. Whether it's verifiable (true/false statements about facts)\n",
    "\n",
    "Post: {text}\n",
    "\n",
    "Return ONLY valid JSON in this exact format (no markdown, no extra text):\n",
    "{{\n",
    "  \"claims\": [\n",
    "    {{\n",
    "      \"claim_id\": \"1\",\n",
    "      \"text\": \"extracted claim\",\n",
    "      \"description\": \"brief description\",\n",
    "      \"is_verifiable\": true\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert fact-checker. Always respond with valid JSON only.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        # Remove markdown code blocks if present\n",
    "        if content.startswith(\"```\"):\n",
    "            content = content.split(\"```\")[1]\n",
    "            if content.startswith(\"json\"):\n",
    "                content = content[4:]\n",
    "        content = content.strip()\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        return result.get(\"claims\", [])\n",
    "    \n",
    "    except Exception as e:\n",
    "        return [{\"error\": str(e)}]\n",
    "\n",
    "# Test claim extraction\n",
    "if client:\n",
    "    claims = extract_claims_llm(sample_post, client)\n",
    "    print(\"Extracted Claims:\")\n",
    "    print(\"=\" * 60)\n",
    "    for claim in claims:\n",
    "        if \"error\" not in claim:\n",
    "            print(f\"\\nClaim {claim.get('claim_id', 'N/A')}:\")\n",
    "            print(f\"  Text: {claim.get('text', 'N/A')}\")\n",
    "            print(f\"  Verifiable: {claim.get('is_verifiable', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"Error: {claim['error']}\")\n",
    "else:\n",
    "    print(\"Skipping LLM test - no API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Persuasion Technique Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Persuasion Techniques:\n",
      "============================================================\n",
      "\n",
      "Technique: FearAppeal\n",
      "  Confidence: 0.9\n",
      "  Explanation: The claim uses the word 'forcing' and the large number '500,000 migrants' to evoke a sense of threat and overwhelm, suggesting a negative and potentially dangerous situation for Germany.\n",
      "\n",
      "Technique: LoadedLanguage\n",
      "  Confidence: 0.85\n",
      "  Explanation: The phrase 'forcing Germany' is emotionally charged and implies coercion and a lack of agency, aiming to provoke a negative emotional response towards the European Union.\n",
      "\n",
      "Technique: Scapegoating\n",
      "  Confidence: 0.7\n",
      "  Explanation: The claim implicitly blames the European Union for a problem (migrant acceptance) and also introduces a conspiracy theory about COVID-19 and the WHO, diverting attention and blaming external entities for perceived issues.\n",
      "\n",
      "Technique: Exaggeration\n",
      "  Confidence: 0.75\n",
      "  Explanation: The specific number '500,000 migrants' presented as a definitive and immediate mandate from the EU could be an exaggeration or misrepresentation of actual policy or proposals, designed to amplify the perceived threat.\n",
      "\n",
      "Technique: AppealToAuthority\n",
      "  Confidence: 0.6\n",
      "  Explanation: While not explicitly citing an authority, the mention of 'Angela Merkel' and 'WHO' attempts to lend credibility to the conspiracy theory, implying that these entities are involved in a hidden agenda.\n"
     ]
    }
   ],
   "source": [
    "PERSUASION_TAXONOMY = {\n",
    "    \"FearAppeal\": \"Using fear or threats to influence behavior or beliefs\",\n",
    "    \"LoadedLanguage\": \"Using emotionally charged words to influence without evidence\",\n",
    "    \"AppealToAuthority\": \"Citing authority figures without proper evidence\",\n",
    "    \"Scapegoating\": \"Unfairly blaming a person or group for problems\",\n",
    "    \"Exaggeration\": \"Overstating or understating facts for effect\",\n",
    "}\n",
    "\n",
    "def detect_persuasion_techniques(claim_text: str, post_context: str, client) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Detect persuasion techniques in a claim using LLM via OpenRouter.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        return [{\"error\": \"OpenRouter client not initialized\"}]\n",
    "    \n",
    "    taxonomy_str = \"\\n\".join([f\"- {k}: {v}\" for k, v in PERSUASION_TAXONOMY.items()])\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this claim for persuasion techniques.\n",
    "\n",
    "Claim: {claim_text}\n",
    "Full Post Context: {post_context}\n",
    "\n",
    "Available techniques:\n",
    "{taxonomy_str}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"techniques\": [\n",
    "    {{\n",
    "      \"type\": \"TechniqueName\",\n",
    "      \"confidence\": 0.85,\n",
    "      \"explanation\": \"Why this technique applies\"\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in rhetoric and propaganda analysis. Always respond with valid JSON only.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        if content.startswith(\"```\"):\n",
    "            content = content.split(\"```\")[1]\n",
    "            if content.startswith(\"json\"):\n",
    "                content = content[4:]\n",
    "        content = content.strip()\n",
    "        \n",
    "        result = json.loads(content)\n",
    "        return result.get(\"techniques\", [])\n",
    "    \n",
    "    except Exception as e:\n",
    "        return [{\"error\": str(e)}]\n",
    "\n",
    "# Test persuasion detection\n",
    "if client:\n",
    "    test_claim = \"The European Union is forcing Germany to accept 500,000 migrants\"\n",
    "    techniques = detect_persuasion_techniques(test_claim, sample_post, client)\n",
    "    \n",
    "    print(\"Detected Persuasion Techniques:\")\n",
    "    print(\"=\" * 60)\n",
    "    for tech in techniques:\n",
    "        if \"error\" not in tech:\n",
    "            print(f\"\\nTechnique: {tech.get('type', 'N/A')}\")\n",
    "            print(f\"  Confidence: {tech.get('confidence', 'N/A')}\")\n",
    "            print(f\"  Explanation: {tech.get('explanation', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"Error: {tech['error']}\")\n",
    "else:\n",
    "    print(\"Skipping LLM test - no API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **spaCy NER**: Fast entity extraction with type classification\n",
    "2. **OpenRouter API**: LLM-based claim extraction and persuasion detection using Gemini\n",
    "\n",
    "### Tool Roles in Our Pipeline:\n",
    "\n",
    "| Task | Tool | Reason |\n",
    "|------|------|--------|\n",
    "| Entity Recognition | spaCy | Fast, consistent, good for standard entities |\n",
    "| Claim Extraction | Gemini (via OpenRouter) | Requires understanding of semantics and context |\n",
    "| Persuasion Detection | Gemini (via OpenRouter) | Requires nuanced understanding of rhetoric |\n",
    "\n",
    "### OpenRouter Configuration\n",
    "\n",
    "To use this notebook, set your OpenRouter API key in `.env`:\n",
    "```\n",
    "OPENROUTER_API_KEY=your-key-here\n",
    "```\n",
    "\n",
    "Get your API key at: https://openrouter.ai/keys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
